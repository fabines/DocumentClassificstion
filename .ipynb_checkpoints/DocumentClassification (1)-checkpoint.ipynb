{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import collections\n",
    "import os\n",
    "import string\n",
    "#nltk.download()\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from itertools import islice\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# part 2:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import metrics\n",
    "import re\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "#warnings.simplifier(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Pre Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the dataset:\n",
    "\n",
    "\n",
    "dataSet=[]\n",
    "data_test=[]\n",
    "\n",
    "# <document,list of words> for trainX -> after being cleaned\n",
    "documentsTrain = {}\n",
    "\n",
    "# <document,list of words> for testX -> after being cleaned\n",
    "documentsTest = {}\n",
    "\n",
    "mostFrequentInCorpus={} # dictionary that contains the most common words in the whole corpus\n",
    "\n",
    "#dictionaries that save data ONLY for the train dataset\n",
    "# <cat,no.docs>\n",
    "catDocs = {} \n",
    "# <cat,no.terms,frequency>\n",
    "catTerms = {}\n",
    "# document and its categories\n",
    "dicCategory = {}\n",
    "#data of the dataset\n",
    "data = ''\n",
    "# vector clean text for each document train:\n",
    "_train_clean_Data = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, we will load the data from the given path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataSet(path):\n",
    "   temp=sklearn.datasets.load_files(path, description=None, categories=None, load_content=True, shuffle=True, encoding=None, random_state=0)\n",
    "   data = temp.data\n",
    "   return temp\n",
    "\n",
    "dataSet = makeDataSet(\"C:\\\\Users\\\\Estif\\\\Downloads\\\\ohsumed-first-20000-docs.tar\\\\ohsumed-first-20000-docs\"\n",
    "            \"\\\\training\")\n",
    "data_test = makeDataSet(\"C:\\\\Users\\\\Estif\\\\Downloads\\\\ohsumed-first-20000-docs.tar\\\\ohsumed-first-20000-docs\"\n",
    "            \"\\\\test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 2 dictionaries\n",
    "def mergeDicts(dic1, dic2):\n",
    "    input = [dic1, dic2]\n",
    "    return sum((Counter(dict(x)) for x in input), Counter())\n",
    "\n",
    "# count dictionary's words frequency\n",
    "def makeFrequencyDic(words):\n",
    "    wordFreq=[words.count(p) for p in words]\n",
    "    return dict(zip(words,wordFreq))\n",
    "    \n",
    "\n",
    "# update frequency for categories\n",
    "def updateCategories(categories,words):\n",
    "    wordsFreq= makeFrequencyDic(words)\n",
    "    for category in categories:\n",
    "        if category in catTerms:\n",
    "            frequencyDic=catTerms[category]\n",
    "            mergedDic=mergeDicts(frequencyDic,wordsFreq)\n",
    "            catTerms[category] = mergedDic\n",
    "        else:\n",
    "            catTerms[category] = wordsFreq\n",
    "            \n",
    "# sort frequency dic in descending order\n",
    "def sortFreqDict(freqdict):\n",
    "    sorted = [(freqdict[key], key) for key in freqdict]\n",
    "    sorted.sort()\n",
    "    sorted.reverse()\n",
    "    return sorted\n",
    "\n",
    "# \"Return first n items of the iterable as a list\"\n",
    "def take(n, iterable):\n",
    "    return list(islice(iterable, n))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Secondly, we will clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanData(dataSet,documents):\n",
    "   counter = 0\n",
    "   for file in dataSet.filenames:\n",
    "       data = dataSet.data[counter]\n",
    "       category = os.path.basename(os.path.dirname(file))\n",
    "       fileName = os.path.basename(file)\n",
    "       documents[fileName] = word_tokenize(data.decode(\"utf-8\"))\n",
    "       counter = counter+1\n",
    "       # <category, num of docs>\n",
    "       if category in catDocs:\n",
    "           catDocs[category] = catDocs[category] +1\n",
    "       else:\n",
    "           catDocs[category] = 1\n",
    "           # <doc,list of categories>\n",
    "       if fileName not in dicCategory:\n",
    "           dicCategory[fileName] = []\n",
    "           dicCategory[fileName].append(category)\n",
    "       else:\n",
    "           dicCategory[fileName].append(category)\n",
    "   # stop words filtration ,lowercase and stemmig\n",
    "   stop_words = stopwords.words('english')\n",
    "   moreStopWords=['p', '+/-', '-/+']\n",
    "   stop_words += set(string.punctuation)\n",
    "   stop_words.extend(moreStopWords)\n",
    "   #stem\n",
    "\n",
    "\n",
    "   for key, value in documents.items():\n",
    "       words = documents[key]\n",
    "       ps = PorterStemmer()\n",
    "       #filtered_sentence = [w for w in words if not w in stop_words]\n",
    "       filtered_sentence = []\n",
    "       for w in words:\n",
    "           w = w.lower()\n",
    "           if w not in stop_words:\n",
    "               if len(w)>1:\n",
    "                   try:\n",
    "                       filtered_sentence.append(ps.stem(w))\n",
    "                   except Exception as inst:\n",
    "                       filtered_sentence.append(w)\n",
    "       documents[key] = filtered_sentence\n",
    "       updateCategories(dicCategory[key],filtered_sentence)\n",
    "       \n",
    "   return documents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#####Function that find the most common words in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findMostFrequentInCorpus():\n",
    "    global mostFrequentInCorpus\n",
    "    mostFrequentInCorpus=sortFreqDict(mostFrequentInCorpus) #sort dic by frequency\n",
    "    top5 = dict((val, key) for (key, val) in mostFrequentInCorpus)\n",
    "    top5 = dict(take(5, top5.items()))\n",
    "    return top5\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "\n",
    "def makeTables(dataSet):\n",
    "    global catTerms\n",
    "    print(catTerms.get('C01'))\n",
    "    global mostFrequentInCorpus\n",
    "    numCat = len(dataSet.target_names)\n",
    "    # print number of categories\n",
    "    print('Number of categories are:', numCat)\n",
    "    # make categories and number of docs for each of them table\n",
    "    sortedCategories=collections.OrderedDict(sorted(catDocs.items()))\n",
    "    categoriesArr = []\n",
    "    for category, numFiles in sortedCategories.items():\n",
    "        categoriesArr.append([category,numFiles])\n",
    "\n",
    "    categorisDocumentsDF=pd.DataFrame(categoriesArr, columns=['Category', 'NumberOfFiles'])\n",
    "    display(categorisDocumentsDF)\n",
    "\n",
    "    # make frequencies table\n",
    "    # first, order by category\n",
    "    catTerms = collections.OrderedDict(sorted(catTerms.items()))\n",
    "\n",
    "    categoriesArr = []\n",
    "    for category, freqDic in catTerms.items():\n",
    "        lineArr = []\n",
    "        lineArr.append(category)\n",
    "        freqDic=sortFreqDict(freqDic) #sort dic by frequency\n",
    "        top10 = dict((val, key) for (key, val) in freqDic)\n",
    "        top10 = dict(take(10, top10.items()))\n",
    "        print(\"category: \"+top10.items())\n",
    "\n",
    "        mostFrequentInCorpus=mergeDicts(top10,mostFrequentInCorpus)\n",
    "        \n",
    "        wordsLine=[]\n",
    "        \n",
    "        for word,freq in top10.items():\n",
    "            termAndfreq=[word,freq]\n",
    "            lineArr.append(termAndfreq)\n",
    "            wordsLine.append(termAndfreq)\n",
    "        \n",
    "        categoryPlot = pd.DataFrame(wordsLine, columns = ['Word', 'Count'])    \n",
    "        ax = categoryPlot.plot.bar(title =\"10 most frequent terms in category: \" + \n",
    "                                          category, x='Word', y='Count')\n",
    "\n",
    "        categoriesArr.append(lineArr)\n",
    "        \n",
    "        \n",
    "        x_offset = -0.4\n",
    "        y_offset = 2\n",
    "        for p in ax.patches:\n",
    "          b = p.get_bbox()\n",
    "          val = b.y1 + b.y0    \n",
    "          ax.annotate(val, ((b.x0 + b.x1)/2 + x_offset, b.y1 + y_offset))\n",
    "        \n",
    "        \n",
    "\n",
    "    termsFreqDF=pd.DataFrame(categoriesArr, columns=['Category','term 1','term 2','term 3','term 4','term 5','term 6','term 7','term 8','term 9','term 10'])\n",
    "    display(termsFreqDF)\n",
    "    df2 = pd.DataFrame(termsFreqDF)\n",
    "    df2.transpose()\n",
    "    df2.style\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Lastly, we will make the requires tables for the data TRAIN set - in order to explore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeFrequentGraph():\n",
    "    global catTerms\n",
    "    catTerms=collections.OrderedDict(sorted(catTerms.items())) # first, order by category\n",
    "    for category, wordsDic in catTerms.items():\n",
    "        words_count_docs = {} # saves how many documents each word is in this category. \n",
    "          \n",
    "        for word in wordsDic.keys(): ## initalize all to 0\n",
    "            words_count_docs[word]=0\n",
    "\n",
    "        for word in wordsDic.keys(): # go through all the words of this category\n",
    "            for doc in dicCategory: \n",
    "                if category in dicCategory[doc] and word in documentsTrain[doc]: # if that word is in that document and if that doucment belongs to this category\n",
    "                  words_count_docs[word]+=1\n",
    "                  \n",
    "        # transform it to DF\n",
    "        lines=[]\n",
    "        for word,numDocs in words_count_docs.items():\n",
    "            line=[word,numDocs]\n",
    "            lines.append(line)\n",
    "            \n",
    "        wordsDocs_DF=pd.DataFrame(lines, columns=['word',category+''])\n",
    "        uniqueCount=set(wordsDocs_DF[category+''])\n",
    "        fig=wordsDocs_DF.hist(column=category+'',bins=len(uniqueCount))\n",
    "        plt.ylabel(\"# terms\")\n",
    "        plt.xlabel(\"Frequency of terms (number of documents)\")\n",
    "        plt.show(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def clean_text(X_text_data):\n",
    "##   stop_words = stopwords.words('english')\n",
    "##   moreStopWords=['p', '+/-', '-/+', 'patients', 'may']\n",
    "##   stop_words += set(string.punctuation)\n",
    "##   stop_words.extend(moreStopWords)\n",
    "##   \n",
    "##\n",
    "##   new_x_data=[]\n",
    "##   for text in X_text_data:\n",
    "##       words = text.split(' ')\n",
    "##       ps = PorterStemmer()\n",
    "##       filtered_sentence = []\n",
    "##       for w in words:\n",
    "##           w = w.lower()\n",
    "##           if w not in stop_words:\n",
    "##               if len(w)>1:\n",
    "##                   try:\n",
    "##                       filtered_sentence.append(p##s.stem(w))\n",
    "##                   except Exception as inst:\n",
    "##                       filtered_sentence.append(w)\n",
    "##       new_x_data.append(' '.join(filtered_sentence))\n",
    "##   return new_x_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset:\n",
    "* Clean Dataset.\n",
    "* Display categories and documents frequency table.\n",
    "* Display categories and 10 most common words frequency table.\n",
    "* Display categories and 10 most common words frequency graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#train\n",
    "documentsTrain = cleanData(dataSet,documentsTrain)\n",
    "\n",
    "makeTables(dataSet)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "text=''\n",
    "#_train_clean_Data=clean_text(_train_clean_Data)\n",
    "\n",
    "#print(len(_train_clean_Data))\n",
    "#for key,value in documentsTrain.items():\n",
    "#    text = #\n",
    "#    _train_clean_Data.append(text)\n",
    " \n",
    " \n",
    "y_train, y_test = dataSet.target, data_test.target   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "makeFrequentGraph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top5=findMostFrequentInCorpus()\n",
    "print('top 5 words in the corpus are:')\n",
    "print(top5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Expected challenges\n",
    "* The first challenge was dealing with frequent words that appeard in multiple categeries. In consequence, the uniqueness of each of the categories was damaged since the same words appeared in most of these categories (The reason for that is because the corpus has one theme - medical world, which result in medical related words that re-appear in most of the corpus's documents). We could call these words \"noise\". Such words are: patients, may and so on. In order to solve this problem, we decided to find the most frequent words in the WHOLE corpus and omit them from the top 10 list of each of the categories. This solution will help us to explore the dataset in more depth and examine the words that are unique to each of the corpus's categories (So we will learn why certian words appear in ONE particular category). \n",
    "We wanted to see if we have congruences between different categories, especially in common words (that are not stop words). If we have common corpus words (that are not stop words) that appear in 2 categories and more, then we would except that these said categories (with the congreuence) will be less distinguished by the model. Furthermore, if there are cateroeis that belong to a specific category, then it will be easier to identify documents that belong to this category. Thesefacts link directly to our next challenge. \n",
    "We decided to display our frequent words (for EACH category) in 2 forms - the first graph displays the 10 top words in each category and its number of appearences in this category. The second graph represents the words frequency in documents in each category (not only the top10, since we want to see in how many documents the most dominent words in each category appears\n",
    "\n",
    "* Another challenge, that is being caused directly from the first one, is finding the amout of common corpus words to ommit. Meaning, we wanted to find how many corpus frequent words we dotn want to include in our top 10 caterogies exploration. On the one hand, removing MANY corpus common words resulted in lower model's accuracy. On the other hand, reducing a low number of corpus common words, resulted in a higher model's accuracy. The second option seemmed more prominenet to us, but this accuracy won't be very faithul since it its model will include many words that appear too many times in corpus (noise). We wanted to find the balance, meaning - finding the right number of common corpus words to omit, so it wont hurt the model's accuracy and still make it reliable so we can estimate the dataset's features in an effactive way. We decided to omit 5 common words as a result. We wrote a function named FindMostCommonCorpusWords that find the 5 most frequent words in the corpus. We then added these words to the stop words list manually.\n",
    "\n",
    "* The last challenge we faced is the appearnces of many \"un real\" words, such as: p, +/- and so on (The stemming and stop words proceess haven't removed these). This resulted in data'set noise since these words appeared in many of the top 10 categories word and wouldn't show their true uniqueness. Therefore, we had to omit such words from the dataset's explorations, by adding them to the stop words list. After doing so, we managed to explore each of the categories in mroe depth and see which REAL words are common to its of these said categories. \n",
    "* for the second part, we face the challenge of finding out which paramter's values are bes. We would also need to understand the definitions of each ot them and their effect of each of the machine learning methods we will examine. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Part 2: Document Classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Use features extractions: bag of words and TD-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building x_test data (that went though the pre processing of part one). It will be used for optimazing the results later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "documentsTest = cleanData(data_test,documentsTest)\n",
    "\n",
    "_test_clean_Data = []\n",
    "\n",
    "# vector clean text for each document test: \n",
    "text=''\n",
    "for key,value in documentsTest.items():\n",
    "    text = ' '.join(value)\n",
    "    _test_clean_Data.append(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Bag of words feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bag of words feature extraction:\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "BOW_train = vectorizer.fit_transform(dataSet.data)\n",
    "\n",
    "BOW_test = vectorizer.transform(data_test.data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-IDF feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF feature extraction:\n",
    "\n",
    "Tfidf_vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "#calculate TF-IDF of terms in documents\n",
    "Tfidf_train = Tfidf_vectorizer.fit_transform(dataSet.data)\n",
    "Tfidf_test = Tfidf_vectorizer.transform(data_test.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we will evaluate SVM and NB machine learnings methods and display it on graph to see the differences between each of them (Without making an optimal model). We will examine 4 combinations: SVM with bag of words, SVM with Tf-IDF, Naive Bayes with bag of words, Naive Bayes with Tf-IDF. \n",
    "It's important to note that we will do so without cleaning the x_train and x_test, since we want to see the predicting accuracy without the cleaning factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluates how good the model is\n",
    "def benchmark(clf, X_train, X_test): \n",
    "    print('_' * 80)\n",
    "    print(\"Training: \")\n",
    "    print(clf)\n",
    "    t0 = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    train_time = time() - t0\n",
    "    print(\"train time: %0.3fs\" % train_time)\n",
    "\n",
    "    t0 = time()\n",
    "    pred = clf.predict(X_test)\n",
    "    test_time = time() - t0\n",
    "    print(\"test time:  %0.3fs\" % test_time)\n",
    "\n",
    "    score = metrics.accuracy_score(y_test, pred)\n",
    "    print(\"accuracy:   %0.3f\" % score)\n",
    "\n",
    "    print()\n",
    "    clf_descr = str(clf).split('(')[0]\n",
    "    return clf_descr, score, train_time, test_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare between the different machine learning evaluations using plot \n",
    "def makePlot (results,FE_name):\n",
    "    indices = np.arange(len(results))\n",
    "\n",
    "    results = [[x[i] for x in results] for i in range(4)]\n",
    "\n",
    "    clf_names, score, training_time, test_time = results\n",
    "    training_time = np.array(training_time) / np.max(training_time)\n",
    "    test_time = np.array(test_time) / np.max(test_time)\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.title(FE_name)\n",
    "    plt.barh(indices, score, .2, label=\"score\", color='navy')\n",
    "    plt.barh(indices + .3, training_time, .2, label=\"training time\",\n",
    "             color='c')\n",
    "    plt.barh(indices + .6, test_time, .2, label=\"test time\", color='darkorange')\n",
    "    plt.yticks(())\n",
    "    plt.legend(loc='best')\n",
    "    plt.subplots_adjust(left=.25)\n",
    "    plt.subplots_adjust(top=.95)\n",
    "    plt.subplots_adjust(bottom=.05)\n",
    "\n",
    "    for i, c in zip(indices, clf_names):\n",
    "        plt.text(-.3, i, c)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Evaluating the models for each of the ML:SVM,Naive Bayes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helps to the models for each of the ML methods\n",
    "def compareMLanFeatuers():\n",
    "    \n",
    "   # 1) This evaluation is done using TF-DF feature extraction\n",
    "   results_Tfidf = []\n",
    "   for clf, name in (\n",
    "           (SGDClassifier(),\"SVM Tf-IDF\"),\n",
    "           (MultinomialNB(),\"Naive Bayes Tf-IDF\")):\n",
    "       print('-' * 80)\n",
    "       print(name)\n",
    "       results_Tfidf.append(benchmark(clf, Tfidf_train,Tfidf_test))\n",
    "   makePlot (results_Tfidf,'Tf_IDF') # display the comparison between of the ML's methods.\n",
    "   \n",
    "   # 2) This evaluation is done using bag of words feature extraction \n",
    "   results_BOW = []\n",
    "   for clf, name in (\n",
    "           (SGDClassifier(),\"SVM Bag of Words\"),\n",
    "           (MultinomialNB(),\"Naive Bayes Bag of Words\")):\n",
    "       print('=' * 80)\n",
    "       print(name,'Bag of words')\n",
    "       results_BOW.append(benchmark(clf, BOW_train,BOW_test))\n",
    "   makePlot (results_BOW ,'Bag of Words')  #display the comparison between of the ML's methods. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SVM Tf-IDF\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 0.645s\n",
      "test time:  0.073s\n",
      "accuracy:   0.439\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Naive Bayes Tf-IDF\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.127s\n",
      "test time:  0.085s\n",
      "accuracy:   0.255\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SVM Bag of Words Bag of words\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
      "       early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
      "       l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=None,\n",
      "       n_iter=None, n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
      "       power_t=0.5, random_state=None, shuffle=True, tol=None,\n",
      "       validation_fraction=0.1, verbose=0, warm_start=False)\n",
      "train time: 0.870s\n",
      "test time:  0.086s\n",
      "accuracy:   0.323\n",
      "\n",
      "================================================================================\n",
      "Naive Bayes Bag of Words Bag of words\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "train time: 0.147s\n",
      "test time:  0.185s\n",
      "accuracy:   0.391\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compareMLanFeatuers() # calls the function to compare the machine learning methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will attempt to find the best parameters for each combinations of ML and feature extractions (4 overall), using pipeline and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function recieves 1 feature extraction and 1 machine learning method, \n",
    "# and performes a classification in order to optimize the results\n",
    "def bestClassifier(featureExtraction, clf, methodResults,parameters_clf,name):\n",
    "    categories=list(catTerms.keys())\n",
    "    _clf = Pipeline([('vect', featureExtraction), ('clf', clf)])\n",
    "    gs_clf = GridSearchCV(_clf, parameters_clf, n_jobs=1)\n",
    "    gs_clf = gs_clf.fit(dataSet.data, dataSet.target)\n",
    "    prediction=gs_clf.predict(data_test.data)\n",
    "    acc=metrics.accuracy_score(data_test.target,prediction)\n",
    "    print(name)\n",
    "    print('-----------------------')\n",
    "    print('Best score: ',gs_clf.best_score_)\n",
    "    print('Accuracy: ',acc)\n",
    "    methodResults.append(gs_clf.best_score_)\n",
    "    methodResults.append(acc)\n",
    "    methodResults.append(gs_clf.best_params_)\n",
    "    print('Best params: ',gs_clf.best_params_)\n",
    "    print()\n",
    "    return methodResults\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM and Bag Of Words\n",
      "-----------------------\n",
      "Best score:  0.3233969136394134\n",
      "Accuracy:  0.3288305976596246\n",
      "Best params:  {'clf__alpha': 1.0, 'vect__max_df': 0.3}\n",
      "\n",
      "SVM and TF-IDf\n",
      "-----------------------\n",
      "Best score:  0.3244512604236557\n",
      "Accuracy:  0.4035969527998115\n",
      "Best params:  {'clf__alpha': 0.01, 'vect__max_df': 1.0}\n",
      "\n",
      "Naive Bayes and Bag Of Words\n",
      "-----------------------\n",
      "Best score:  0.314482890827183\n",
      "Accuracy:  0.40234037540249745\n",
      "Best params:  {'clf__alpha': 1.0, 'vect__max_df': 0.3}\n",
      "\n",
      "Naive Bayes and TF-IDF\n",
      "-----------------------\n",
      "Best score:  0.263586696060577\n",
      "Accuracy:  0.4029686641011545\n",
      "Best params:  {'clf__alpha': 0.01, 'vect__max_df': 0.3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# take the best results of each of the machine learning methods\n",
    "SVM_method_results = [] \n",
    "NB_method_results = []\n",
    "parameters_clf = {\n",
    "  'vect__max_df': (0.3,0.4,0.5,0.6,0.7,0.75, 1.0),'clf__alpha': (0.0001, 0.01,1.0)\n",
    "}\n",
    "\n",
    "# SVM + bag of words\n",
    "\n",
    "SVM_method_results=bestClassifier(CountVectorizer(), SGDClassifier(),\n",
    "                                  SVM_method_results, parameters_clf,'SVM and Bag Of Words')\n",
    "# SVM + td-idf\n",
    "\n",
    "SVM_method_results=bestClassifier(TfidfVectorizer(), SGDClassifier(),\n",
    "                                  SVM_method_results, parameters_clf,'SVM and TF-IDf')\n",
    "# Naive bayes + bag of words\n",
    "\n",
    "NB_method_results=bestClassifier(CountVectorizer(), MultinomialNB(),\n",
    "                                  NB_method_results, parameters_clf,'Naive Bayes and Bag Of Words')\n",
    "# Naive bayes + td-idf\n",
    "\n",
    "NB_method_results = bestClassifier(TfidfVectorizer(), MultinomialNB(), \n",
    "                                NB_method_results, parameters_clf,'Naive Bayes and TF-IDF')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## As of now, here are the results we've gotten by the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.32310936451643824, 0.32710280373831774, {'clf__alpha': 1.0, 'vect__max_df': 0.3}, 0.3211923703632704, 0.4113720254456923, {'clf__alpha': 0.01, 'vect__max_df': 1.0}]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ML method</th>\n",
       "      <th>Feature Extraction</th>\n",
       "      <th>Grid Search best score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Best params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.323109</td>\n",
       "      <td>0.327103</td>\n",
       "      <td>{'clf__alpha': 1.0, 'vect__max_df': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Tf-IDF</td>\n",
       "      <td>0.321192</td>\n",
       "      <td>0.411372</td>\n",
       "      <td>{'clf__alpha': 0.01, 'vect__max_df': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.314483</td>\n",
       "      <td>0.402340</td>\n",
       "      <td>{'clf__alpha': 1.0, 'vect__max_df': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Tf-IDf</td>\n",
       "      <td>0.263587</td>\n",
       "      <td>0.402969</td>\n",
       "      <td>{'clf__alpha': 0.01, 'vect__max_df': 0.3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ML method</th>\n",
       "      <th>Feature Extraction</th>\n",
       "      <th>Grid Search best score</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Best params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.323109</td>\n",
       "      <td>0.327103</td>\n",
       "      <td>{'clf__alpha': 1.0, 'vect__max_df': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SVM</td>\n",
       "      <td>Tf-IDF</td>\n",
       "      <td>0.321192</td>\n",
       "      <td>0.411372</td>\n",
       "      <td>{'clf__alpha': 0.01, 'vect__max_df': 1.0}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Bag of Words</td>\n",
       "      <td>0.314483</td>\n",
       "      <td>0.402340</td>\n",
       "      <td>{'clf__alpha': 1.0, 'vect__max_df': 0.3}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Tf-IDf</td>\n",
       "      <td>0.263587</td>\n",
       "      <td>0.402969</td>\n",
       "      <td>{'clf__alpha': 0.01, 'vect__max_df': 0.3}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 0,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "unite_best_score = []\n",
    "unite_acc = []\n",
    "unite_best_params = []\n",
    "\n",
    "print(SVM_method_results)\n",
    "unite_best_score.append(SVM_method_results[0])\n",
    "unite_best_score.append(SVM_method_results[3])\n",
    "unite_best_score.append(NB_method_results[0])\n",
    "unite_best_score.append(NB_method_results[3])\n",
    "\n",
    "unite_acc.append(SVM_method_results[1])\n",
    "unite_acc.append(SVM_method_results[4])\n",
    "unite_acc.append(NB_method_results[1])\n",
    "unite_acc.append(NB_method_results[4])\n",
    "\n",
    "unite_best_params.append(SVM_method_results[2])\n",
    "unite_best_params.append(SVM_method_results[5])\n",
    "unite_best_params.append(NB_method_results[2])\n",
    "unite_best_params.append(NB_method_results[5])\n",
    "\n",
    "\n",
    "\n",
    "results_DF = pd.DataFrame()\n",
    "results_DF['ML method'] = ['SVM','SVM','Naive Bayes', 'Naive Bayes']\n",
    "results_DF['Feature Extraction'] = ['Bag of Words', 'Tf-IDF', 'Bag of Words', 'Tf-IDf']\n",
    "results_DF['Grid Search best score'] = unite_best_score\n",
    "results_DF['Accuracy'] = unite_acc\n",
    "results_DF['Best params'] = unite_best_params\n",
    "\n",
    "display(results_DF)       \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will tune the parameters with best params results we got using the pipeline. Furthermore, we will use CLEANED data train and data test (using the pre processing we did in part 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# function that shows the comparison matrix in a PLOT, so we can examine in it in a more depth\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "  \n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.get_axes()\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    "\n",
    "def cm2inch(*tupl):\n",
    "    '''\n",
    "    Specify figure size in centimeter in matplotlib\n",
    "    Source: https://stackoverflow.com/a/22787457/395857\n",
    "    By gns-ank\n",
    "    '''\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "\n",
    "\n",
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "    '''\n",
    "    Inspired by:\n",
    "    - https://stackoverflow.com/a/16124677/395857 \n",
    "    - https://stackoverflow.com/a/25074150/395857\n",
    "    '''\n",
    "\n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    "\n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    #fig.set_size_inches(cm2inch(40, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
    "\n",
    "\n",
    "\n",
    "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
    "    '''\n",
    "    Plot scikit-learn classification report.\n",
    "    Extension based on https://stackoverflow.com/a/31689645/395857 \n",
    "    '''\n",
    "    lines = classification_report.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    support = []\n",
    "    class_names = []\n",
    "    for line in lines[2 : (len(lines) - 2)]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(t[0])\n",
    "        plotMat.append(v)\n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### With the optimal parameters/feature extraction, we will evaluate each of the machine learning methods once again, in order to get better results. Furthermore, we will CLEAN the x_train and x_test this time, in order to see if it also helps to get the model more accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Classification Evaluation (report)\n",
    "from sklearn.metrics import classification_report\n",
    "def tableResults(X_train,X_test, clf, name):\n",
    "        categories=list(catTerms.keys())\n",
    "        clf.fit(X_train,y_train) \n",
    "        report = classification_report(y_test,  clf.predict(X_test), \n",
    "                                       target_names=categories)\n",
    "        print('---'+name+'---')\n",
    "        print(report)\n",
    "        plot_classification_report(report)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves the evaluations of each machine learning method, with it's \n",
    "# optimal parameters values and chosen feature extraction \n",
    "evaluations = []\n",
    "def evaluateOptimal(FE_with_params, clf_with_params, methods_names):\n",
    "    global _train_clean_Data\n",
    "    global _test_clean_Data\n",
    "    _train_clean_Data = np.asarray(_train_clean_Data)\n",
    "    _test_clean_Data=np.asarray(_test_clean_Data)\n",
    "\n",
    "    _train_clean_Data=_train_clean_Data.reshape(-1, 1)\n",
    "    _test_clean_Data =_test_clean_Data.reshape(-1, 1)\n",
    "    featureExtraction = FE_with_params\n",
    "    train = featureExtraction.fit_transform(dataSet.data) \n",
    "    test = featureExtraction.transform(data_test.data)\n",
    "    clf_method = clf_with_params\n",
    "    print(methods_names)\n",
    "    evaluations.append(benchmark(clf_method, train, test))\n",
    "    tableResults(_train_clean_Data,_test_clean_Data,clf_method,methods_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with bag of words\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "train time: 0.085s\n",
      "test time:  0.059s\n",
      "accuracy:   0.413\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-8301dbdc05aa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mFE_withparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mclf_with_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mevaluateOptimal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFE_withparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf_with_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'Naive Bayes with bag of words'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# running Naive Bayes with TF-IDF - optimal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-ab187ed815e1>\u001b[0m in \u001b[0;36mevaluateOptimal\u001b[1;34m(FE_with_params, clf_with_params, methods_names)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethods_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mevaluations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbenchmark\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf_method\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mtableResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_train_clean_Data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_test_clean_Data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclf_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmethods_names\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-fe9724e0abf8>\u001b[0m in \u001b[0;36mtableResults\u001b[1;34m(X_train, X_test, clf, name)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtableResults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcategories\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcatTerms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         report = classification_report(y_test,  clf.predict(X_test), \n\u001b[0;32m      8\u001b[0m                                        target_names=categories)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\naive_bayes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mself\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m         \"\"\"\n\u001b[1;32m--> 585\u001b[1;33m         \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    586\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    754\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 756\u001b[1;33m                     estimator=estimator)\n\u001b[0m\u001b[0;32m    757\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    580\u001b[0m                              \u001b[1;34m\" minimum of %d is required%s.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    581\u001b[0m                              % (n_samples, shape_repr, ensure_min_samples,\n\u001b[1;32m--> 582\u001b[1;33m                                 context))\n\u001b[0m\u001b[0;32m    583\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    584\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_features\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# running Naive Bayes with bag of words - optimal\n",
    "FE_withparams = CountVectorizer(max_df=0.3, stop_words='english')\n",
    "clf_with_params = MultinomialNB(alpha=0.1)\n",
    "evaluateOptimal(FE_withparams, clf_with_params,'Naive Bayes with bag of words')\n",
    "\n",
    "# running Naive Bayes with TF-IDF - optimal\n",
    "FE_withparams = CountVectorizer(max_df=0.3, stop_words='english')\n",
    "clf_with_params = MultinomialNB(alpha=0.1)\n",
    "evaluateOptimal(FE_withparams, clf_with_params,'Naive Bayes with Tf-IDF')\n",
    "\n",
    "# running SVM with TF-IDF - optimal\n",
    "FE_withparams = TfidfVectorizer(sublinear_tf='True', max_df=0.5, stop_words='english')\n",
    "clf_with_params = SGDClassifier(alpha=0.01)\n",
    "evaluateOptimal(FE_withparams, clf_with_params,'SVM with TF-IDF')\n",
    "\n",
    "# running SVM with Bag Of Words - optimal\n",
    "FE_withparams = TfidfVectorizer(sublinear_tf='True', max_df=0.3, stop_words='english')\n",
    "clf_with_params = SGDClassifier( alpha=0.1)\n",
    "evaluateOptimal(FE_withparams, clf_with_params,'SVM with Bag Of Words')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy matrix of each feature extraction and machine learning methods we examined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Tf-IDF\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True)\n",
      "train time: 0.056s\n",
      "test time:  0.032s\n",
      "accuracy:   0.413\n",
      "\n",
      "---Naive Bayes with Tf-IDF---\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        C19       0.35      0.47      0.40       506\n",
      "        C04       0.29      0.30      0.30       233\n",
      "        C20       0.65      0.34      0.45        70\n",
      "        C23       0.50      0.62      0.55      1467\n",
      "        C08       0.39      0.43      0.41       429\n",
      "        C14       0.44      0.52      0.47       632\n",
      "        C21       0.34      0.17      0.23       146\n",
      "        C07       0.37      0.33      0.35       600\n",
      "        C02       0.35      0.29      0.32       129\n",
      "        C10       0.44      0.48      0.45       941\n",
      "        C18       0.43      0.47      0.44       202\n",
      "        C01       0.50      0.38      0.43       548\n",
      "        C06       0.40      0.42      0.41       386\n",
      "        C13       0.54      0.63      0.58      1301\n",
      "        C22       0.31      0.33      0.32       320\n",
      "        C12       0.28      0.25      0.26       228\n",
      "        C17       0.36      0.42      0.39       348\n",
      "        C15       0.38      0.50      0.43       400\n",
      "        C16       0.32      0.22      0.26       191\n",
      "        C09       0.36      0.42      0.39       695\n",
      "        C11       0.42      0.49      0.45       717\n",
      "        C03       0.10      0.11      0.10        91\n",
      "        C05       0.30      0.15      0.20      2153\n",
      "\n",
      "avg / total       0.40      0.41      0.40     12733\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The best model we selected is *****---------sdsdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The reason for choosing this particular model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
